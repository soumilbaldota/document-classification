{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfbb9a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy-wordnet in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: spacy>=2 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy-wordnet) (3.5.1)\n",
      "Requirement already satisfied: nltk<3.6,>=3.3 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy-wordnet) (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from nltk<3.6,>=3.3->spacy-wordnet) (4.64.1)\n",
      "Requirement already satisfied: regex in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from nltk<3.6,>=3.3->spacy-wordnet) (2022.10.31)\n",
      "Requirement already satisfied: joblib in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from nltk<3.6,>=3.3->spacy-wordnet) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from nltk<3.6,>=3.3->spacy-wordnet) (8.1.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (8.1.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (65.6.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (3.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\soumi\\appdata\\roaming\\python\\python39\\site-packages (from spacy>=2->spacy-wordnet) (2.28.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (1.22.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (0.10.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (1.1.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (0.7.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\soumi\\appdata\\roaming\\python\\python39\\site-packages (from spacy>=2->spacy-wordnet) (23.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 23.0 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (2.4.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (1.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (1.10.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from spacy>=2->spacy-wordnet) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=2->spacy-wordnet) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soumi\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soumi\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\soumi\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (1.26.14)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy>=2->spacy-wordnet) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy>=2->spacy-wordnet) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from tqdm->nltk<3.6,>=3.3->spacy-wordnet) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soumi\\.conda\\envs\\tf\\lib\\site-packages (from jinja2->spacy>=2->spacy-wordnet) (2.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy-wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782fd20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172b42de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\soumi\\\\Desktop\\\\SRMIST_OD203SRM_Document_classification\\\\ML_DL Notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef1547a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "os.getcwd()\n",
    "entertainment=  pd.read_csv('../Dataset/Entertainment/Entertainment_Dataset.csv')\n",
    "entertainment.columns = ['unnamed', 'values', 'category']\n",
    "entertainment = entertainment.drop('unnamed', axis = 1)\n",
    "\n",
    "insurance=  pd.read_csv('../Dataset/insurance/insurance_dataset.csv')\n",
    "insurance.columns = ['unnamed', 'values', 'category']\n",
    "insurance = insurance.drop('unnamed', axis = 1)\n",
    "\n",
    "finance=  pd.read_csv('../Dataset/finance/finance_dataset.csv')\n",
    "finance.columns = ['unnamed', 'values', 'category']\n",
    "finance = finance.drop('unnamed', axis = 1)\n",
    "\n",
    "travel = pd.read_csv('../Dataset/travel/Travel_Dataset.csv')\n",
    "travel.columns = ['unnamed', 'values', 'category']\n",
    "travel = travel.drop('unnamed', axis = 1)\n",
    "\n",
    "medical = pd.read_csv('../Dataset/Medical/medical_dataset.csv')\n",
    "medical.columns = ['unnamed', 'values', 'category']\n",
    "medical = medical.drop('unnamed', axis = 1)\n",
    "\n",
    "l = [insurance, entertainment, finance, travel, medical]\n",
    "df = pd.concat(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76bb8d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'insurance': 1000,\n",
       "         'entertainment': 386,\n",
       "         'finance': 44999,\n",
       "         nan: 10,\n",
       "         'travel': 957,\n",
       "         'medical': 13200})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "Counter(df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d190a200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading wordnet: HTTP Error 404: Not Found\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error loading wordnet: HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\pattern\\text\\en\\wordnet\\__init__.py:57\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcorpora/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\nltk\\data.py:585\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    584\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[1;32m--> 585\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\soumi/nltk_data'\n    - 'C:\\\\Users\\\\soumi\\\\.conda\\\\envs\\\\tf\\\\nltk_data'\n    - 'C:\\\\Users\\\\soumi\\\\.conda\\\\envs\\\\tf\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\soumi\\\\.conda\\\\envs\\\\tf\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\soumi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\pattern\\text\\en\\wordnet\\__init__.py:60\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_on_error\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# Sometimes there are problems with the default index.xml URL. Then we will try this...\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\nltk\\downloader.py:784\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_on_error:\n\u001b[1;32m--> 784\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m halt_on_error:\n",
      "\u001b[1;31mValueError\u001b[0m: Error loading wordnet: <urlopen error [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextgenie\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextGenie\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      3\u001b[0m textgenie \u001b[38;5;241m=\u001b[39m TextGenie(\n\u001b[0;32m      4\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhetpandya/t5-small-tapaco\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m                     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m                      )\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\textgenie\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtextgenie\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextGenie\n\u001b[0;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.9.7\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m __author__    \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHet Pandya\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\textgenie\\textgenie.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrammar_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pass2act, is_passive\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5ForConditionalGeneration, T5Tokenizer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m punctuation\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\textgenie\\grammar_utils.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01men\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lexeme\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\pattern\\text\\en\\__init__.py:79\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodality\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     74\u001b[0m     mood, INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE,\n\u001b[0;32m     75\u001b[0m     modality, uncertain, EPISTEMIC,\n\u001b[0;32m     76\u001b[0m     negated\n\u001b[0;32m     77\u001b[0m )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Import all submodules.\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inflect\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordlist\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\pattern\\text\\en\\__init__.py:80\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Import all submodules.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inflect\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordlist\n\u001b[0;32m     83\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\pattern\\text\\en\\wordnet\\__init__.py:65\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Downloader \u001b[38;5;28;01mas\u001b[39;00m NLTKDownloader\n\u001b[0;32m     64\u001b[0m             d \u001b[38;5;241m=\u001b[39m NLTKDownloader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://nltk.github.com/nltk_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m             \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_on_error\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Use the Brown corpus for calculating information content (IC)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m brown_ic \u001b[38;5;241m=\u001b[39m wn_ic\u001b[38;5;241m.\u001b[39mic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mic-brown.dat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\nltk\\downloader.py:784\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    782\u001b[0m show(msg\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_on_error:\n\u001b[1;32m--> 784\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m halt_on_error:\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Error loading wordnet: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "\n",
    "from textgenie import TextGenie\n",
    "from tqdm import tqdm\n",
    "textgenie = TextGenie(\n",
    "                    \"hetpandya/t5-small-tapaco\",\n",
    "                    \"bert-base-uncased\",\n",
    "                    \"en_core_web_sm\",\n",
    "                    device = 'cuda:0'\n",
    "                     )\n",
    "\n",
    "def augmentor(dataset):\n",
    "    es = list(eval(f\"{dataset}['values']\"))\n",
    "    aug = []\n",
    "    for x in tqdm(range(len(es))):\n",
    "        es[x] = es[x].replace('\\n', ' ')\n",
    "        es[x] = es[x].replace(\"\\'\", ' ')\n",
    "        try:\n",
    "            l = textgenie.magic_once(\n",
    "                                    f\"{es[x]}\",\n",
    "                                    \"paraphrase: \",\n",
    "                                    n_paraphrase_predictions=2 ,\n",
    "                                    n_mask_predictions=1,\n",
    "                                    convert_to_active = False\n",
    "                                    )\n",
    "            \n",
    "            aug.extend(l)\n",
    "        except:\n",
    "            continue\n",
    "    es.extend(aug)\n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a6ef12",
   "metadata": {},
   "source": [
    "# It  takes about 12 hours to run below code on GPU !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40d1a82a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ent = augmentor('entertainment')\n",
    "# tra = augmentor('travel')\n",
    "# ins = augmentor('insurance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b26713e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_dataset(listname, category):\n",
    "#     a = eval(listname)\n",
    "#     df = pd.DataFrame(zip(a, [category]*len(a)) ,columns = ['values', 'category'])\n",
    "#     return df\n",
    "# to_dataset('ent', 'entertainment').to_csv('Dataset/Entertainment/entertainment_aug.csv')\n",
    "# to_dataset('tra', 'travel').to_csv('Dataset/Travel/travel_aug.csv')\n",
    "# to_dataset('ins', 'insurance').to_csv('Dataset/Insurance/insurance_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d409a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entertainment = pd.read_csv('../Dataset/Entertainment/entertainment_aug.csv')\n",
    "entertainment.columns = ['unnamed', 'values', 'category']\n",
    "entertainment = entertainment.drop('unnamed', axis = 1)\n",
    "\n",
    "travel = pd.read_csv('../Dataset/Travel/travel_aug.csv')\n",
    "travel.columns = ['unnamed', 'values', 'category']\n",
    "travel = travel.drop('unnamed', axis = 1)\n",
    "\n",
    "insurance = pd.read_csv('../Dataset/Insurance/insurance_aug.csv')\n",
    "insurance.columns = ['unnamed', 'values', 'category']\n",
    "insurance = insurance.drop('unnamed', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a72e107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>values</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gallery unveils interactive tree  A Christmas ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jarre joins fairytale celebration  French musi...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Musical treatment for Capra film  The classic ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Richard and Judy choose top books  The 10 auth...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Poppins musical gets flying start  The stage a...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20872</th>\n",
       "      <td>buffy creator joins wonder woman the creator o...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20873</th>\n",
       "      <td>buffy creator joins wonder woman the creator o...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20874</th>\n",
       "      <td>buffy creator joins wonder woman the creator o...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20875</th>\n",
       "      <td>buffy creator joins wonder woman the creator o...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20876</th>\n",
       "      <td>buffy creator joins wonder woman the creator o...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20877 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  values       category\n",
       "0      Gallery unveils interactive tree  A Christmas ...  entertainment\n",
       "1      Jarre joins fairytale celebration  French musi...  entertainment\n",
       "2      Musical treatment for Capra film  The classic ...  entertainment\n",
       "3      Richard and Judy choose top books  The 10 auth...  entertainment\n",
       "4      Poppins musical gets flying start  The stage a...  entertainment\n",
       "...                                                  ...            ...\n",
       "20872  buffy creator joins wonder woman the creator o...  entertainment\n",
       "20873  buffy creator joins wonder woman the creator o...  entertainment\n",
       "20874  buffy creator joins wonder woman the creator o...  entertainment\n",
       "20875  buffy creator joins wonder woman the creator o...  entertainment\n",
       "20876  buffy creator joins wonder woman the creator o...  entertainment\n",
       "\n",
       "[20877 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entertainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2a7aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [insurance, entertainment, finance, travel, medical]\n",
    "df = pd.concat(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f971760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>values</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>can you borrow against globe Life Insurancebor...</td>\n",
       "      <td>insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do Medicare cover my spouseif your spouse have...</td>\n",
       "      <td>insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what happen when you change homeowner insuranc...</td>\n",
       "      <td>insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what be a typical renter insurance costI be su...</td>\n",
       "      <td>insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what be car insurance base oncar insurance rat...</td>\n",
       "      <td>insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13195</th>\n",
       "      <td>([\"Intact function of the Forkhead Box P2 (FOX...</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13196</th>\n",
       "      <td>([\"Studies on ADHD in educational settings ind...</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13197</th>\n",
       "      <td>(['The mechanisms underlying cerebellar learni...</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13198</th>\n",
       "      <td>(['Withania somnifera root extract has been us...</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13199</th>\n",
       "      <td>(['Deep brain stimulation (DBS) has been found...</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143338 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  values   category\n",
       "0      can you borrow against globe Life Insurancebor...  insurance\n",
       "1      do Medicare cover my spouseif your spouse have...  insurance\n",
       "2      what happen when you change homeowner insuranc...  insurance\n",
       "3      what be a typical renter insurance costI be su...  insurance\n",
       "4      what be car insurance base oncar insurance rat...  insurance\n",
       "...                                                  ...        ...\n",
       "13195  ([\"Intact function of the Forkhead Box P2 (FOX...    medical\n",
       "13196  ([\"Studies on ADHD in educational settings ind...    medical\n",
       "13197  (['The mechanisms underlying cerebellar learni...    medical\n",
       "13198  (['Withania somnifera root extract has been us...    medical\n",
       "13199  (['Deep brain stimulation (DBS) has been found...    medical\n",
       "\n",
       "[143338 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a3def6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'finance': 44999, 'travel': 38451, 'insurance': 25801, 'entertainment': 20877, 'medical': 13200, nan: 10})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(df['category']))\n",
    "categories = list(dict(Counter(df['category'])).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9dcbc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insurance', 'entertainment', 'finance', nan, 'travel', 'medical']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0139d063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        can you borrow against globe Life Insurancebor...\n",
       "1        do Medicare cover my spouseif your spouse have...\n",
       "2        what happen when you change homeowner insuranc...\n",
       "3        what be a typical renter insurance costI be su...\n",
       "4        what be car insurance base oncar insurance rat...\n",
       "                               ...                        \n",
       "13195    ([\"Intact function of the Forkhead Box P2 (FOX...\n",
       "13196    ([\"Studies on ADHD in educational settings ind...\n",
       "13197    (['The mechanisms underlying cerebellar learni...\n",
       "13198    (['Withania somnifera root extract has been us...\n",
       "13199    (['Deep brain stimulation (DBS) has been found...\n",
       "Name: values, Length: 143338, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6fcbebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143338, 196738)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['values'])\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X = tfidf_transformer.fit_transform(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dca1ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = {categories[x]:x for x in range(len(categories))}\n",
    "\n",
    "y_true = []\n",
    "for x in df['category']:\n",
    "    y_true.append(mp[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff4c63c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143338,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_true = np.array(y_true)\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fc994f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSplitEqualBinary(X, y, samples_n): #samples_n per class\n",
    "    \n",
    "    indicesClass1 = []\n",
    "    indicesClass2 = []\n",
    "    \n",
    "    for i in range(0, len(y)):\n",
    "        if y[i] == 0 and len(indicesClass1) < samples_n:\n",
    "            indicesClass1.append(i)\n",
    "        elif y[i] == 1 and len(indicesClass2) < samples_n:\n",
    "            indicesClass2.append(i)\n",
    "            \n",
    "        if len(indicesClass1) == samples_n and len(indicesClass2) == samples_n:\n",
    "            break\n",
    "    \n",
    "    X_test_class1 = X[indicesClass1]\n",
    "    X_test_class2 = X[indicesClass2]\n",
    "    \n",
    "    X_test = np.concatenate((X_test_class1,X_test_class2), axis=0)\n",
    "    \n",
    "    #remove x_test from X\n",
    "    X_train = np.delete(X, indicesClass1 + indicesClass2, axis=0)\n",
    "    \n",
    "    Y_test_class1 = y[indicesClass1]\n",
    "    Y_test_class2 = y[indicesClass2]\n",
    "    \n",
    "    y_test = np.concatenate((Y_test_class1,Y_test_class2), axis=0)\n",
    "    \n",
    "    #remove y_test from y\n",
    "    y_train = np.delete(y, indicesClass1 + indicesClass2, axis=0)\n",
    "    \n",
    "    if (X_test.shape[0] != 2 * samples_n or y_test.shape[0] != 2 * samples_n):\n",
    "        raise Exception(\"Problem with split 1!\")\n",
    "        \n",
    "    if (X_train.shape[0] + X_test.shape[0] != X.shape[0] or y_train.shape[0] + y_test.shape[0] != y.shape[0]):\n",
    "        raise Exception(\"Problem with split 2!\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "008f137e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mTrainSplitEqualBinary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 18\u001b[0m, in \u001b[0;36mTrainSplitEqualBinary\u001b[1;34m(X, y, samples_n)\u001b[0m\n\u001b[0;32m     15\u001b[0m X_test_class1 \u001b[38;5;241m=\u001b[39m X[indicesClass1]\n\u001b[0;32m     16\u001b[0m X_test_class2 \u001b[38;5;241m=\u001b[39m X[indicesClass2]\n\u001b[1;32m---> 18\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_class1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_test_class2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#remove x_test from X\u001b[39;00m\n\u001b[0;32m     21\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(X, indicesClass1 \u001b[38;5;241m+\u001b[39m indicesClass2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = TrainSplitEqualBinary(X, y_true, samples_n=3300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8799eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "clf = LogisticRegression(max_iter = 300)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171bb5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e29885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8320242993294364\n",
      "roc_auc_score: 0.9283707493431214\n",
      "confusion matrix: \n",
      " [[5140    0    6    0    1    1]\n",
      " [   0 4285    3    0    0    0]\n",
      " [   4    0 8847    0    0    0]\n",
      " [   0    0    2    0    0    0]\n",
      " [   3    1   15    0 7779    0]\n",
      " [   0    0    7    0    0 2574]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import balanced_accuracy_score as bas\n",
    "from sklearn.metrics import roc_auc_score as ras\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = bas(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "conf_m = cm(y_test, y_pred)\n",
    "\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "accuracy = ras(y_test, y_pred, multi_class='ovr')\n",
    "print(f\"roc_auc_score: {accuracy}\")\n",
    "\n",
    "print(f\"confusion matrix: \\n {conf_m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7effc2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90cff72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7933499810118777\n",
      "roc_auc_score: 0.9154886641358875\n",
      "confusion matrix: \n",
      " [[5143    0    0    0    5    0]\n",
      " [  23 4233    1    0   31    0]\n",
      " [  58    0 7275    0 1511    7]\n",
      " [   0    0    1    0    1    0]\n",
      " [   0    0    0    0 7798    0]\n",
      " [  75    1   32    0   16 2457]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = bas(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "conf_m = cm(y_test, y_pred)\n",
    "\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "accuracy = ras(y_test, y_pred, multi_class='ovr')\n",
    "print(f\"roc_auc_score: {accuracy}\")\n",
    "\n",
    "print(f\"confusion matrix: \\n {conf_m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12bffe04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC())"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "clf = CalibratedClassifierCV(svm.LinearSVC())\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c768036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9158080072141971\n",
      "roc_auc_score: 0.9998033333674085\n",
      "confusion matrix: \n",
      " [[5145    0    1    0    1    1]\n",
      " [   0 4288    0    0    0    0]\n",
      " [   3    0 8847    0    0    1]\n",
      " [   0    0    1    1    0    0]\n",
      " [   4    1   12    0 7781    0]\n",
      " [   0    0    5    0    0 2576]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = bas(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "conf_m = cm(y_test, y_pred)\n",
    "\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "accuracy = ras(y_test, y_pred, multi_class='ovr')\n",
    "print(f\"roc_auc_score: {accuracy}\")\n",
    "\n",
    "print(f\"confusion matrix: \\n {conf_m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e72d8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=Pipeline(steps=[('standardscaler',\n",
       "                                                       StandardScaler(with_mean=False)),\n",
       "                                                      ('sgdclassifier',\n",
       "                                                       SGDClassifier())]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "clf = CalibratedClassifierCV(make_pipeline(StandardScaler(with_mean=False),\n",
    "                    SGDClassifier(max_iter=1000, tol=1e-3)))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf280f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8038519033641837\n",
      "roc_auc_score: 0.9564457044123554\n",
      "confusion matrix: \n",
      " [[5093    1   50    0    3    1]\n",
      " [   3 4134  147    0    4    0]\n",
      " [   0    5 8827    0   19    0]\n",
      " [   0    0    2    0    0    0]\n",
      " [   8    1   13    0 7776    0]\n",
      " [   1    3  317    0    1 2259]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = bas(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "conf_m = cm(y_test, y_pred)\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "accuracy = ras(y_test, y_pred, multi_class='ovr')\n",
    "print(f\"roc_auc_score: {accuracy}\")\n",
    "\n",
    "print(f\"confusion matrix: \\n {conf_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a8494",
   "metadata": {},
   "source": [
    "# Best accuracy is given by the LinearSVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb3da9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC())"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "clf = CalibratedClassifierCV(svm.LinearSVC())\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bab9ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "travel\n",
      "finance\n",
      "insurance\n",
      "entertainment\n",
      "medical\n",
      "finance\n",
      "travel\n",
      "travel\n",
      "travel\n"
     ]
    }
   ],
   "source": [
    "new_data = [\n",
    "            'flights from mumbai to japan',\n",
    "           '5000 rupees dollar to be transferred tommorow',\n",
    "           'health insurance is realy expensive',\n",
    "           'the Lord Of The Rings is a great movie',\n",
    "           'liver surgery is needed',\n",
    "           'The largest organ on the body is the skin also known as derma',\n",
    "            'the train is late again',\n",
    "            'let us get on the bus',\n",
    "            'we can go to the train station using the car',\n",
    "           ]\n",
    "X_new = vectorizer.transform(new_data)\n",
    "X_new = tfidf_transformer.transform(X_new)\n",
    "y_new = clf.predict(X_new)\n",
    "for x in y_new:\n",
    "    print(categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3926133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in above sentence =  105\n",
      "finance\n"
     ]
    }
   ],
   "source": [
    "finance_news = ['Also known as a credit document.\\\n",
    "                This term usually refers to the main documents in a financing\\\n",
    "                transaction under which an obligor owes financial obligations to\\\n",
    "                (or which otherwise create a liability in favour of) a lender (or lenders),\\\n",
    "                agent, arranger or other secured party (for example, a swap counterparty).\\\n",
    "                It is often used as a defined term and, although the parties may designate\\\n",
    "                any document a finance document, it typically includes: Facility agreements.\\\n",
    "                Security documents (such as mortgages and charges). Guarantees and indemnities.\\\n",
    "                Agreements appointing security trustees. Swaps and other derivative contracts.\\\n",
    "                Priority and ranking agreements (such as intercreditor agreements). Drawdown requests.\\\n",
    "                Fee letters.']\n",
    "\n",
    "print('Number of words in above sentence = ', len(finance_news[0].split()))\n",
    "X_new = vectorizer.transform(finance_news)\n",
    "X_new = tfidf_transformer.transform(X_new)\n",
    "y_new = clf.predict(X_new)\n",
    "for x in y_new:\n",
    "    print(categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0116e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in above sentence =  112\n",
      "travel\n"
     ]
    }
   ],
   "source": [
    "travel_news = [\"Firstly, you might want to consider visiting some\\\n",
    "                of India's most popular destinations such as Delhi, Agra, and Jaipur,\\\n",
    "                collectively known as the Golden Triangle. These cities are filled \\\n",
    "                with stunning monuments, palaces, and forts that will give you a glimpse\\\n",
    "                into India's rich history and culture. In Delhi, you could visit the Red Fort,\\\n",
    "                India Gate, Lotus Temple, and the Qutub Minar, to name just a few of the many \\\n",
    "                amazing sights in the city. Agra is home to the iconic Taj Mahal, one of the\\\n",
    "                Seven Wonders of the World, and also the Agra Fort. In Jaipur, you could explore\\\n",
    "                the beautiful City Palace, Hawa Mahal, and the Amber Fort.\"]\n",
    "\n",
    "print('Number of words in above sentence = ', len(travel_news[0].split()))\n",
    "X_new = vectorizer.transform(travel_news)\n",
    "X_new = tfidf_transformer.transform(X_new)\n",
    "y_new = clf.predict(X_new)\n",
    "for x in y_new:\n",
    "    print(categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c45e21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in above sentence =  231\n",
      "medical\n"
     ]
    }
   ],
   "source": [
    "medical = [\"The heart is a muscular organ located \\\n",
    "            in the chest cavity that is responsible for pumping\\\n",
    "            blood throughout the body. The heart has four chambers \\\n",
    "            - two atria and two ventricles - that work together to \\\n",
    "            circulate blood. The process of how the heart functions \\\n",
    "            can be described in the following steps: Deoxygenated blood\\\n",
    "            from the body enters the right atrium through the superior\\\n",
    "            and inferior vena cava. The right atrium contracts, pushing \\\n",
    "            blood through the tricuspid valve into the right ventricle. \\\n",
    "            The right ventricle contracts, pumping blood through the pulmonary\\\n",
    "            valve into the pulmonary artery, which carries the blood to the lungs.\\\n",
    "            In the lungs, the blood receives oxygen and releases carbon dioxide.\\\n",
    "            Oxygenated blood from the lungs returns to the heart via the pulmonary\\\n",
    "            veins, entering the left atrium. The left atrium contracts, pushing blood \\\n",
    "            through the mitral valve into the left ventricle. The left \\\n",
    "            ventricle contracts, pumping blood through the aortic valve into\\\n",
    "            the aorta, which carries the blood to the rest of the body. The \\\n",
    "            cycle repeats as deoxygenated blood returns to the right atrium. \\\n",
    "            The heart's pumping action is controlled by electrical impulses\\\n",
    "            that originate in the sinoatrial (SA) node, a specialized group \\\n",
    "            of cells in the right atrium. These electrical impulses spread \\\n",
    "            through the heart, causing the chambers to contract in a coordinated \\\n",
    "            manner, which results in an efficient circulation of \\\n",
    "            blood throughout the body.\"]\n",
    "\n",
    "print('Number of words in above sentence = ', len(medical[0].split()))\n",
    "X_new = vectorizer.transform(medical)\n",
    "X_new = tfidf_transformer.transform(X_new)\n",
    "y_new = clf.predict(X_new)\n",
    "for x in y_new:\n",
    "    print(categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63da6bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in above sentence =  342\n",
      "insurance\n"
     ]
    }
   ],
   "source": [
    "insurance = [\"Insurance is a contract \\\n",
    "            between an individual or entity (policyholder) \\\n",
    "            and an insurance company (insurer) in which the\\\n",
    "            policyholder pays a premium in exchange for protection\\\n",
    "            against specific risks or losses. Here are some\\\n",
    "            important facts about insurance: Types of Insurance: \\\n",
    "            There are many types of insurance, including health\\\n",
    "            insurance, life insurance, car insurance, home insurance,\\\n",
    "            and business insurance, among others. Premiums: \\\n",
    "            Insurance policies require the policyholder to pay\\\n",
    "            a premium, which is a periodic payment (monthly,\\\n",
    "            quarterly, annually, etc.) to maintain the policy.\\\n",
    "            Premiums are typically based on the level of risk\\\n",
    "            associated with the insured individual or entity.\\\n",
    "            Deductibles: A deductible is the amount the\\\n",
    "            policyholder is responsible for paying before \\\n",
    "            insurance coverage kicks in. For example, in car\\\n",
    "            insurance, if you have a $500 deductible and you \\\n",
    "            get into an accident that causes $1,500 worth \\\n",
    "            of damage, you would pay the first $500 and \\\n",
    "            the insurance company would cover the remaining\\\n",
    "            $1,000. Coverage Limits: Insurance policies also\\\n",
    "            have coverage limits, which are the maximum amounts\\\n",
    "            that the insurance company will pay out in the\\\n",
    "            event of a covered loss or claim. Policyholders \\\n",
    "            can choose coverage limits based on their \\\n",
    "            individual needs and budget. Claims: In the\\\n",
    "            event of a loss or claim, the policyholder must\\\n",
    "            file a claim with the insurance company. The insurer\\\n",
    "            will investigate the claim and determine if it is \\\n",
    "            covered under the policy. If approved, the insurer will\\\n",
    "            provide compensation or reimbursement for the loss, up \\\n",
    "            to the coverage limit. Benefits: Insurance provides financial\\\n",
    "            protection and peace of mind for policyholders in the \\\n",
    "            event of unexpected losses or damages. It can also help\\\n",
    "            to mitigate risks associated with certain activities or\\\n",
    "            situations, such as driving a car or owning a home.\\\n",
    "            Legal Requirements: Some types of insurance are legally \\\n",
    "            required, such as car insurance in most states, while others \\\n",
    "            are optional but highly recommended, such as health insurance.\\\n",
    "            It's important to understand your insurance policy and the level\\\n",
    "            of coverage it provides to ensure you have the appropriate protection\\\n",
    "            in place for your specific needs.\"]\n",
    "\n",
    "print('Number of words in above sentence = ', len(insurance[0].split()))\n",
    "X_new = vectorizer.transform(insurance)\n",
    "X_new = tfidf_transformer.transform(X_new)\n",
    "y_new = clf.predict(X_new)\n",
    "for x in y_new:\n",
    "    print(categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae1767d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in above sentence =  218\n",
      "entertainment\n"
     ]
    }
   ],
   "source": [
    "entertainment = [\"Titanic is a classic movie that was released in \\\n",
    "                1997 and directed by James Cameron. The film is a fictionalized \\\n",
    "                account of the tragic sinking of the Titanic, a luxury liner that \\\n",
    "                hit an iceberg and sank on its maiden voyage in 1912. The movie is\\\n",
    "                a visually stunning masterpiece that tells a compelling love story\\\n",
    "                between two passengers from different social classes who fall in\\\n",
    "                love aboard the ship. The performances by the lead actors, Leonardo\\\n",
    "                DiCaprio and Kate Winslet, are exceptional and their chemistry on-screen\\\n",
    "                is truly captivating. The film's cinematography and special effects are\\\n",
    "                also impressive, especially in the scenes depicting the sinking of the\\\n",
    "                ship, which are both dramatic and heartbreaking. One of the strengths\\\n",
    "                of the film is its attention to historical detail and accuracy, from \\\n",
    "                the design of the ship to the costumes worn by the characters. The film \\\n",
    "                also sheds light on the social dynamics of the time, highlighting the class\\\n",
    "                divisions that existed and the struggles faced by immigrants seeking a better\\\n",
    "                life in America. Overall, Titanic is a classic and unforgettable film that has \\\n",
    "                stood the test of time. Its masterful storytelling, stunning visuals, \\\n",
    "                and powerful performances make it a must-see for any movie lover, and its \\\n",
    "                themes of love, loss, and sacrifice continue to resonate with audiences today.\"]\n",
    "print('Number of words in above sentence = ', len(entertainment[0].split()))\n",
    "X_new = vectorizer.transform(entertainment)\n",
    "X_new = tfidf_transformer.transform(X_new)\n",
    "y_new = clf.predict(X_new)\n",
    "for x in y_new:\n",
    "    print(categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c24a626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(vectorizer)\n",
    "tfidf_transformer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b9239418",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('vectorizer', 'wb')\n",
    "pickle.dump(vectorizer, file)\n",
    "\n",
    "file = open('tfidf_transformer', 'wb')\n",
    "pickle.dump(tfidf_transformer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ceb57518",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vectorizer', 'wb') as fp:\n",
    "    pickle.dump(vectorizer, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab59a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_transformer', 'wb') as fp:\n",
    "    pickle.dump(tfidf_transformer, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7315aeb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
