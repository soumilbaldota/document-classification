{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dfbb9a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy-wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "782fd20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "172b42de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\soumi\\\\Desktop\\\\SRMIST_OD203SRM_Document_classification\\\\ML_DL Notebooks'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cef1547a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "os.getcwd()\n",
    "entertainment=  pd.read_csv('../Dataset/Entertainment/Entertainment_Dataset.csv')\n",
    "entertainment.columns = ['unnamed', 'values', 'category']\n",
    "entertainment = entertainment.drop('unnamed', axis = 1)\n",
    "\n",
    "insurance=  pd.read_csv('../Dataset/insurance/insurance_dataset.csv')\n",
    "insurance.columns = ['unnamed', 'values', 'category']\n",
    "insurance = insurance.drop('unnamed', axis = 1)\n",
    "\n",
    "finance=  pd.read_csv('../Dataset/finance/finance_dataset.csv')\n",
    "finance.columns = ['unnamed', 'values', 'category']\n",
    "finance = finance.drop('unnamed', axis = 1)\n",
    "\n",
    "travel = pd.read_csv('../Dataset/travel/Travel_Dataset.csv')\n",
    "travel.columns = ['unnamed', 'values', 'category']\n",
    "travel = travel.drop('unnamed', axis = 1)\n",
    "\n",
    "medical = pd.read_csv('../Dataset/Medical/medical_dataset.csv')\n",
    "medical.columns = ['unnamed', 'values', 'category']\n",
    "medical = medical.drop('unnamed', axis = 1)\n",
    "\n",
    "l = [insurance, entertainment, finance, travel, medical]\n",
    "df = pd.concat(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "76bb8d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'insurance': 1000,\n",
       "         'entertainment': 386,\n",
       "         'finance': 44999,\n",
       "         nan: 10,\n",
       "         'travel': 957,\n",
       "         'medical': 13200})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "Counter(df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d190a200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Paraphrase Model..\n",
      "Loading Mask Fill Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from textgenie import TextGenie\n",
    "from tqdm import tqdm\n",
    "textgenie = TextGenie(\n",
    "                    \"hetpandya/t5-small-tapaco\",\n",
    "                    \"bert-base-uncased\",\n",
    "                    \"en_core_web_sm\",\n",
    "                    device = 'cuda:0'\n",
    "                     )\n",
    "\n",
    "def augmentor(dataset):\n",
    "    es = list(eval(f\"{dataset}['values']\"))\n",
    "    aug = []\n",
    "    for x in tqdm(range(len(es))):\n",
    "        es[x] = es[x].replace('\\n', ' ')\n",
    "        es[x] = es[x].replace(\"\\'\", ' ')\n",
    "        try:\n",
    "            l = textgenie.magic_once(\n",
    "                                    f\"{es[x]}\",\n",
    "                                    \"paraphrase: \",\n",
    "                                    n_paraphrase_predictions=2 ,\n",
    "                                    n_mask_predictions=1,\n",
    "                                    convert_to_active = False\n",
    "                                    )\n",
    "            \n",
    "            aug.extend(l)\n",
    "        except:\n",
    "            continue\n",
    "    es.extend(aug)\n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a6ef12",
   "metadata": {},
   "source": [
    "# It  takes about 12 hours to run below code on GPU !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "40d1a82a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ent = augmentor('entertainment')\n",
    "# tra = augmentor('travel')\n",
    "# ins = augmentor('insurance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b26713e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_dataset(listname, category):\n",
    "#     a = eval(listname)\n",
    "#     df = pd.DataFrame(zip(a, [category]*len(a)) ,columns = ['values', 'category'])\n",
    "#     return df\n",
    "# to_dataset('ent', 'entertainment').to_csv('Dataset/Entertainment/entertainment_aug.csv')\n",
    "# to_dataset('tra', 'travel').to_csv('Dataset/Travel/travel_aug.csv')\n",
    "# to_dataset('ins', 'insurance').to_csv('Dataset/Insurance/insurance_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "84d409a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entertainment = pd.read_csv('../Dataset/Entertainment/entertainment_aug.csv')\n",
    "entertainment.columns = ['unnamed', 'values', 'category']\n",
    "entertainment = entertainment.drop('unnamed', axis = 1)\n",
    "\n",
    "travel = pd.read_csv('../Dataset/Travel/travel_aug.csv')\n",
    "travel.columns = ['unnamed', 'values', 'category']\n",
    "travel = travel.drop('unnamed', axis = 1)\n",
    "\n",
    "insurance = pd.read_csv('../Dataset/Insurance/insurance_aug.csv')\n",
    "insurance.columns = ['unnamed', 'values', 'category']\n",
    "insurance = insurance.drop('unnamed', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4a72e107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>values</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gallery unveils interactive tree  A Christmas ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jarre joins fairytale celebration  French musi...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Musical treatment for Capra film  The classic ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Richard and Judy choose top books  The 10 auth...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Poppins musical gets flying start  The stage a...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20872</th>\n",
       "      <td>buffy creator joins wonder woman the creator o...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20873</th>\n",
       "      <td>buffy creator joins wonder woman the creator o...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20874</th>\n",
       "      <td>buffy creator joins wonder woman the creator o...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20875</th>\n",
       "      <td>buffy creator joins wonder woman the creator o...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20876</th>\n",
       "      <td>buffy creator joins wonder woman the creator o...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20877 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  values       category\n",
       "0      Gallery unveils interactive tree  A Christmas ...  entertainment\n",
       "1      Jarre joins fairytale celebration  French musi...  entertainment\n",
       "2      Musical treatment for Capra film  The classic ...  entertainment\n",
       "3      Richard and Judy choose top books  The 10 auth...  entertainment\n",
       "4      Poppins musical gets flying start  The stage a...  entertainment\n",
       "...                                                  ...            ...\n",
       "20872  buffy creator joins wonder woman the creator o...  entertainment\n",
       "20873  buffy creator joins wonder woman the creator o...  entertainment\n",
       "20874  buffy creator joins wonder woman the creator o...  entertainment\n",
       "20875  buffy creator joins wonder woman the creator o...  entertainment\n",
       "20876  buffy creator joins wonder woman the creator o...  entertainment\n",
       "\n",
       "[20877 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entertainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d2a7aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [insurance, entertainment, finance, travel, medical]\n",
    "df = pd.concat(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1f971760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>values</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>can you borrow against globe Life Insurancebor...</td>\n",
       "      <td>insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do Medicare cover my spouseif your spouse have...</td>\n",
       "      <td>insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what happen when you change homeowner insuranc...</td>\n",
       "      <td>insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what be a typical renter insurance costI be su...</td>\n",
       "      <td>insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what be car insurance base oncar insurance rat...</td>\n",
       "      <td>insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13195</th>\n",
       "      <td>([\"Intact function of the Forkhead Box P2 (FOX...</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13196</th>\n",
       "      <td>([\"Studies on ADHD in educational settings ind...</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13197</th>\n",
       "      <td>(['The mechanisms underlying cerebellar learni...</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13198</th>\n",
       "      <td>(['Withania somnifera root extract has been us...</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13199</th>\n",
       "      <td>(['Deep brain stimulation (DBS) has been found...</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143338 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  values   category\n",
       "0      can you borrow against globe Life Insurancebor...  insurance\n",
       "1      do Medicare cover my spouseif your spouse have...  insurance\n",
       "2      what happen when you change homeowner insuranc...  insurance\n",
       "3      what be a typical renter insurance costI be su...  insurance\n",
       "4      what be car insurance base oncar insurance rat...  insurance\n",
       "...                                                  ...        ...\n",
       "13195  ([\"Intact function of the Forkhead Box P2 (FOX...    medical\n",
       "13196  ([\"Studies on ADHD in educational settings ind...    medical\n",
       "13197  (['The mechanisms underlying cerebellar learni...    medical\n",
       "13198  (['Withania somnifera root extract has been us...    medical\n",
       "13199  (['Deep brain stimulation (DBS) has been found...    medical\n",
       "\n",
       "[143338 rows x 2 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2a3def6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'finance': 44999, 'travel': 38451, 'insurance': 25801, 'entertainment': 20877, 'medical': 13200, nan: 10})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(df['category']))\n",
    "categories = list(dict(Counter(df['category'])).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d9dcbc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insurance', 'entertainment', 'finance', nan, 'travel', 'medical']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "00b0c40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\soumi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "def clean_text(text_list):\n",
    "    # Create stemmer and lemmatizer objects\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Define stop words to remove\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    cleaned_texts = []\n",
    "    for text in tqdm(text_list):\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Tokenize text into individual words\n",
    "        words = word_tokenize(text)\n",
    "\n",
    "        # Remove stop words\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # Perform stemming on each word\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "        # Perform lemmatization on each word\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "        # Join cleaned words back into a string\n",
    "        cleaned_text = \" \".join(words)\n",
    "\n",
    "        # Append cleaned text to list\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "\n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "af47e454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 143338/143338 [11:58<00:00, 199.51it/s]\n"
     ]
    }
   ],
   "source": [
    "k = clean_text(df['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d6fcbebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143338, 175793)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(k)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dca1ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = {categories[x]:x for x in range(len(categories))}\n",
    "\n",
    "y_true = []\n",
    "for x in df['category']:\n",
    "    y_true.append(mp[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ff4c63c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143338,)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_true = np.array(y_true)\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "008f137e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_true, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8799eb9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m clf \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1589\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1587\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1588\u001b[0m     prefer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocesses\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1589\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1592\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1593\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1617\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:806\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    803\u001b[0m     iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[0;32m    804\u001b[0m         np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[0;32m    805\u001b[0m     ]\n\u001b[1;32m--> 806\u001b[0m     opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    814\u001b[0m     n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    815\u001b[0m         solver,\n\u001b[0;32m    816\u001b[0m         opt_res,\n\u001b[0;32m    817\u001b[0m         max_iter,\n\u001b[0;32m    818\u001b[0m         extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    819\u001b[0m     )\n\u001b[0;32m    820\u001b[0m     w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\scipy\\optimize\\_minimize.py:623\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    621\u001b[0m                               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    624\u001b[0m                             callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    627\u001b[0m                          \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py:360\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    354\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 360\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:267\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 267\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:233\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 233\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:134\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\scipy\\optimize\\optimize.py:74\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     73\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\scipy\\optimize\\optimize.py:68\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 68\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:774\u001b[0m, in \u001b[0;36m_logistic_regression_path.<locals>.func\u001b[1;34m(x, *args)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_multinomial_loss_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:348\u001b[0m, in \u001b[0;36m_multinomial_loss_grad\u001b[1;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m fit_intercept \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m n_classes \u001b[38;5;241m*\u001b[39m (n_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    347\u001b[0m grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n_classes, n_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mbool\u001b[39m(fit_intercept)), dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m--> 348\u001b[0m loss, p, w \u001b[38;5;241m=\u001b[39m \u001b[43m_multinomial_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m sample_weight[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m    350\u001b[0m diff \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m (p \u001b[38;5;241m-\u001b[39m Y)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:299\u001b[0m, in \u001b[0;36m_multinomial_loss\u001b[1;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    297\u001b[0m p \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, w\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    298\u001b[0m p \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m intercept\n\u001b[1;32m--> 299\u001b[0m p \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m    300\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(sample_weight \u001b[38;5;241m*\u001b[39m Y \u001b[38;5;241m*\u001b[39m p)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    301\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m alpha \u001b[38;5;241m*\u001b[39m squared_norm(w)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\scipy\\special\\_logsumexp.py:99\u001b[0m, in \u001b[0;36mlogsumexp\u001b[1;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[0;32m     96\u001b[0m         a \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.\u001b[39m  \u001b[38;5;66;03m# promote to at least float\u001b[39;00m\n\u001b[0;32m     97\u001b[0m         a[b \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m---> 99\u001b[0m a_max \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamax\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a_max\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    102\u001b[0m     a_max[\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misfinite(a_max)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2791\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2675\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2677\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2678\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2679\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2789\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2790\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2792\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "clf = LogisticRegression(max_iter = 300)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e29885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8320242993294364\n",
      "roc_auc_score: 0.9283707493431214\n",
      "confusion matrix: \n",
      " [[5140    0    6    0    1    1]\n",
      " [   0 4285    3    0    0    0]\n",
      " [   4    0 8847    0    0    0]\n",
      " [   0    0    2    0    0    0]\n",
      " [   3    1   15    0 7779    0]\n",
      " [   0    0    7    0    0 2574]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import balanced_accuracy_score as bas\n",
    "from sklearn.metrics import roc_auc_score as ras\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = bas(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "conf_m = cm(y_test, y_pred)\n",
    "\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "accuracy = ras(y_test, y_pred, multi_class='ovr')\n",
    "print(f\"roc_auc_score: {accuracy}\")\n",
    "\n",
    "print(f\"confusion matrix: \\n {conf_m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7effc2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90cff72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7933499810118777\n",
      "roc_auc_score: 0.9154886641358875\n",
      "confusion matrix: \n",
      " [[5143    0    0    0    5    0]\n",
      " [  23 4233    1    0   31    0]\n",
      " [  58    0 7275    0 1511    7]\n",
      " [   0    0    1    0    1    0]\n",
      " [   0    0    0    0 7798    0]\n",
      " [  75    1   32    0   16 2457]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = bas(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "conf_m = cm(y_test, y_pred)\n",
    "\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "accuracy = ras(y_test, y_pred, multi_class='ovr')\n",
    "print(f\"roc_auc_score: {accuracy}\")\n",
    "\n",
    "print(f\"confusion matrix: \\n {conf_m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "12bffe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soumi\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC())"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "clf = CalibratedClassifierCV(svm.LinearSVC())\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c768036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8135498222566092\n",
      "roc_auc_score: 0.9736912785864749\n",
      "confusion matrix: \n",
      " [[5120    0    7    0   21    0]\n",
      " [   1 4217   29    0   41    0]\n",
      " [   1    0 8338    0  512    0]\n",
      " [   0    0    1    0    1    0]\n",
      " [   0    0    0    0 7798    0]\n",
      " [   1    0   70    0   29 2481]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = bas(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "conf_m = cm(y_test, y_pred)\n",
    "\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "accuracy = ras(y_test, y_pred, multi_class='ovr')\n",
    "print(f\"roc_auc_score: {accuracy}\")\n",
    "\n",
    "print(f\"confusion matrix: \\n {conf_m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e72d8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=Pipeline(steps=[('standardscaler',\n",
       "                                                       StandardScaler(with_mean=False)),\n",
       "                                                      ('sgdclassifier',\n",
       "                                                       SGDClassifier())]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "clf = CalibratedClassifierCV(make_pipeline(StandardScaler(with_mean=False),\n",
    "                    SGDClassifier(max_iter=1000, tol=1e-3)))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf280f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7621857839053714\n",
      "roc_auc_score: 0.9558095141725853\n",
      "confusion matrix: \n",
      " [[5091    3   48    0    6    0]\n",
      " [   1 4196   81    0   10    0]\n",
      " [   1    1 8057    0  792    0]\n",
      " [   0    0    2    0    0    0]\n",
      " [   5    2    6    0 7785    0]\n",
      " [   3    2  776    0    1 1799]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = bas(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "conf_m = cm(y_test, y_pred)\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "accuracy = ras(y_test, y_pred, multi_class='ovr')\n",
    "print(f\"roc_auc_score: {accuracy}\")\n",
    "\n",
    "print(f\"confusion matrix: \\n {conf_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a8494",
   "metadata": {},
   "source": [
    "# Best accuracy is given by the LinearSVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0bab9ffb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 175793 features, but TfidfTransformer is expecting 196738 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m new_data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflights from mumbai to japan\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5000 rupees dollar to be transferred tommorow\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe can go to the train station using the car\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m            ]\n\u001b[0;32m     12\u001b[0m X_new \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(new_data)\n\u001b[1;32m---> 13\u001b[0m X_new \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m y_new \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_new)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m y_new:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1660\u001b[0m, in \u001b[0;36mTfidfTransformer.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform a count matrix to a tf or tf-idf representation.\u001b[39;00m\n\u001b[0;32m   1645\u001b[0m \n\u001b[0;32m   1646\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;124;03m        Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1660\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1664\u001b[0m         X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 175793 features, but TfidfTransformer is expecting 196738 features as input."
     ]
    }
   ],
   "source": [
    "new_data = [\n",
    "            'flights from mumbai to japan',\n",
    "           '5000 rupees dollar to be transferred tommorow',\n",
    "           'health insurance is realy expensive',\n",
    "           'the Lord Of The Rings is a great movie',\n",
    "           'liver surgery is needed',\n",
    "           'The largest organ on the body is the skin also known as derma',\n",
    "            'the train is late again',\n",
    "            'let us get on the bus',\n",
    "            'we can go to the train station using the car',\n",
    "           ]\n",
    "X_new = vectorizer.transform(new_data)\n",
    "X_new = tfidf_transformer.transform(X_new)\n",
    "y_new = clf.predict(X_new)\n",
    "for x in y_new:\n",
    "    print(categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3926133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in above sentence =  105\n",
      "finance\n"
     ]
    }
   ],
   "source": [
    "finance_news = ['Also known as a credit document.\\\n",
    "                This term usually refers to the main documents in a financing\\\n",
    "                transaction under which an obligor owes financial obligations to\\\n",
    "                (or which otherwise create a liability in favour of) a lender (or lenders),\\\n",
    "                agent, arranger or other secured party (for example, a swap counterparty).\\\n",
    "                It is often used as a defined term and, although the parties may designate\\\n",
    "                any document a finance document, it typically includes: Facility agreements.\\\n",
    "                Security documents (such as mortgages and charges). Guarantees and indemnities.\\\n",
    "                Agreements appointing security trustees. Swaps and other derivative contracts.\\\n",
    "                Priority and ranking agreements (such as intercreditor agreements). Drawdown requests.\\\n",
    "                Fee letters.']\n",
    "\n",
    "print('Number of words in above sentence = ', len(finance_news[0].split()))\n",
    "X_new = vectorizer.transform(finance_news)\n",
    "X_new = tfidf_transformer.transform(X_new)\n",
    "y_new = clf.predict(X_new)\n",
    "for x in y_new:\n",
    "    print(categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0116e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in above sentence =  112\n",
      "travel\n"
     ]
    }
   ],
   "source": [
    "travel_news = [\"Firstly, you might want to consider visiting some\\\n",
    "                of India's most popular destinations such as Delhi, Agra, and Jaipur,\\\n",
    "                collectively known as the Golden Triangle. These cities are filled \\\n",
    "                with stunning monuments, palaces, and forts that will give you a glimpse\\\n",
    "                into India's rich history and culture. In Delhi, you could visit the Red Fort,\\\n",
    "                India Gate, Lotus Temple, and the Qutub Minar, to name just a few of the many \\\n",
    "                amazing sights in the city. Agra is home to the iconic Taj Mahal, one of the\\\n",
    "                Seven Wonders of the World, and also the Agra Fort. In Jaipur, you could explore\\\n",
    "                the beautiful City Palace, Hawa Mahal, and the Amber Fort.\"]\n",
    "\n",
    "print('Number of words in above sentence = ', len(travel_news[0].split()))\n",
    "X_new = vectorizer.transform(finance_news)\n",
    "X_new = tfidf_transformer.transform(X_new)\n",
    "y_new = clf.predict(X_new)\n",
    "for x in y_new:\n",
    "    print(categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c45e21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in above sentence =  231\n",
      "medical\n"
     ]
    }
   ],
   "source": [
    "medical = [\"The heart is a muscular organ located \\\n",
    "            in the chest cavity that is responsible for pumping\\\n",
    "            blood throughout the body. The heart has four chambers \\\n",
    "            - two atria and two ventricles - that work together to \\\n",
    "            circulate blood. The process of how the heart functions \\\n",
    "            can be described in the following steps: Deoxygenated blood\\\n",
    "            from the body enters the right atrium through the superior\\\n",
    "            and inferior vena cava. The right atrium contracts, pushing \\\n",
    "            blood through the tricuspid valve into the right ventricle. \\\n",
    "            The right ventricle contracts, pumping blood through the pulmonary\\\n",
    "            valve into the pulmonary artery, which carries the blood to the lungs.\\\n",
    "            In the lungs, the blood receives oxygen and releases carbon dioxide.\\\n",
    "            Oxygenated blood from the lungs returns to the heart via the pulmonary\\\n",
    "            veins, entering the left atrium. The left atrium contracts, pushing blood \\\n",
    "            through the mitral valve into the left ventricle. The left \\\n",
    "            ventricle contracts, pumping blood through the aortic valve into\\\n",
    "            the aorta, which carries the blood to the rest of the body. The \\\n",
    "            cycle repeats as deoxygenated blood returns to the right atrium. \\\n",
    "            The heart's pumping action is controlled by electrical impulses\\\n",
    "            that originate in the sinoatrial (SA) node, a specialized group \\\n",
    "            of cells in the right atrium. These electrical impulses spread \\\n",
    "            through the heart, causing the chambers to contract in a coordinated \\\n",
    "            manner, which results in an efficient circulation of \\\n",
    "            blood throughout the body.\"]\n",
    "\n",
    "print('Number of words in above sentence = ', len(medical[0].split()))\n",
    "X_new = vectorizer.transform(medical)\n",
    "X_new = tfidf_transformer.transform(X_new)\n",
    "y_new = clf.predict(X_new)\n",
    "for x in y_new:\n",
    "    print(categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "63da6bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in above sentence =  342\n",
      "insurance\n"
     ]
    }
   ],
   "source": [
    "insurance = [\"Insurance is a contract \\\n",
    "            between an individual or entity (policyholder) \\\n",
    "            and an insurance company (insurer) in which the\\\n",
    "            policyholder pays a premium in exchange for protection\\\n",
    "            against specific risks or losses. Here are some\\\n",
    "            important facts about insurance: Types of Insurance: \\\n",
    "            There are many types of insurance, including health\\\n",
    "            insurance, life insurance, car insurance, home insurance,\\\n",
    "            and business insurance, among others. Premiums: \\\n",
    "            Insurance policies require the policyholder to pay\\\n",
    "            a premium, which is a periodic payment (monthly,\\\n",
    "            quarterly, annually, etc.) to maintain the policy.\\\n",
    "            Premiums are typically based on the level of risk\\\n",
    "            associated with the insured individual or entity.\\\n",
    "            Deductibles: A deductible is the amount the\\\n",
    "            policyholder is responsible for paying before \\\n",
    "            insurance coverage kicks in. For example, in car\\\n",
    "            insurance, if you have a $500 deductible and you \\\n",
    "            get into an accident that causes $1,500 worth \\\n",
    "            of damage, you would pay the first $500 and \\\n",
    "            the insurance company would cover the remaining\\\n",
    "            $1,000. Coverage Limits: Insurance policies also\\\n",
    "            have coverage limits, which are the maximum amounts\\\n",
    "            that the insurance company will pay out in the\\\n",
    "            event of a covered loss or claim. Policyholders \\\n",
    "            can choose coverage limits based on their \\\n",
    "            individual needs and budget. Claims: In the\\\n",
    "            event of a loss or claim, the policyholder must\\\n",
    "            file a claim with the insurance company. The insurer\\\n",
    "            will investigate the claim and determine if it is \\\n",
    "            covered under the policy. If approved, the insurer will\\\n",
    "            provide compensation or reimbursement for the loss, up \\\n",
    "            to the coverage limit. Benefits: Insurance provides financial\\\n",
    "            protection and peace of mind for policyholders in the \\\n",
    "            event of unexpected losses or damages. It can also help\\\n",
    "            to mitigate risks associated with certain activities or\\\n",
    "            situations, such as driving a car or owning a home.\\\n",
    "            Legal Requirements: Some types of insurance are legally \\\n",
    "            required, such as car insurance in most states, while others \\\n",
    "            are optional but highly recommended, such as health insurance.\\\n",
    "            It's important to understand your insurance policy and the level\\\n",
    "            of coverage it provides to ensure you have the appropriate protection\\\n",
    "            in place for your specific needs.\"]\n",
    "\n",
    "print('Number of words in above sentence = ', len(insurance[0].split()))\n",
    "X_new = vectorizer.transform(insurance)\n",
    "X_new = tfidf_transformer.transform(X_new)\n",
    "y_new = clf.predict(X_new)\n",
    "for x in y_new:\n",
    "    print(categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae1767d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in above sentence =  218\n",
      "entertainment\n"
     ]
    }
   ],
   "source": [
    "entertainment = [\"Titanic is a classic movie that was released in \\\n",
    "                1997 and directed by James Cameron. The film is a fictionalized \\\n",
    "                account of the tragic sinking of the Titanic, a luxury liner that \\\n",
    "                hit an iceberg and sank on its maiden voyage in 1912. The movie is\\\n",
    "                a visually stunning masterpiece that tells a compelling love story\\\n",
    "                between two passengers from different social classes who fall in\\\n",
    "                love aboard the ship. The performances by the lead actors, Leonardo\\\n",
    "                DiCaprio and Kate Winslet, are exceptional and their chemistry on-screen\\\n",
    "                is truly captivating. The film's cinematography and special effects are\\\n",
    "                also impressive, especially in the scenes depicting the sinking of the\\\n",
    "                ship, which are both dramatic and heartbreaking. One of the strengths\\\n",
    "                of the film is its attention to historical detail and accuracy, from \\\n",
    "                the design of the ship to the costumes worn by the characters. The film \\\n",
    "                also sheds light on the social dynamics of the time, highlighting the class\\\n",
    "                divisions that existed and the struggles faced by immigrants seeking a better\\\n",
    "                life in America. Overall, Titanic is a classic and unforgettable film that has \\\n",
    "                stood the test of time. Its masterful storytelling, stunning visuals, \\\n",
    "                and powerful performances make it a must-see for any movie lover, and its \\\n",
    "                themes of love, loss, and sacrifice continue to resonate with audiences today.\"]\n",
    "print('Number of words in above sentence = ', len(entertainment[0].split()))\n",
    "X_new = vectorizer.transform(entertainment)\n",
    "X_new = tfidf_transformer.transform(X_new)\n",
    "y_new = clf.predict(X_new)\n",
    "for x in y_new:\n",
    "    print(categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae8622c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
